{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN - Lab 1b: Logistic Regression #\n",
    "\n",
    "Tudor Berariu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_clusters(N, variance=0.1):\n",
    "    N1 = N // 2\n",
    "    N2 = N - N1\n",
    "    X1 = np.random.randn(N1, 2) * variance + 0.25\n",
    "    X2 = np.random.randn(N2, 2) * variance + 0.75\n",
    "    X = np.concatenate([X1, X2], axis=0)\n",
    "    T = np.concatenate([np.zeros((N1)), np.ones((N2))], axis=0)\n",
    "    p = np.random.permutation(N)\n",
    "    return X[p], T[p]\n",
    "\n",
    "def two_rings(N, separation=0.1, noise=0.2):\n",
    "    N1 = N // 2\n",
    "    N2 = N - N1\n",
    "    angles = np.random.rand(N, 1) * 2 * np.pi\n",
    "    radius = np.random.rand(N, 1) + np.random.randn(N, 1) * noise\n",
    "    radius *= .5 - separation / 2\n",
    "    radius[N2:] += .5 + separation / 2\n",
    "    X = np.concatenate([radius * np.sin(angles), radius * np.cos(angles)], axis=1)\n",
    "    T = np.concatenate([np.zeros((N1)), np.ones((N2))], axis=0)\n",
    "    p = np.random.permutation(N)\n",
    "    return X[p], T[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(X, T, Y=None):\n",
    "    m = [\"x\", \"o\"]  # Indicates true class\n",
    "    col = [\"red\", \"blue\"]  # Indicates prediction\n",
    "    fig = plt.figure(); plt.axis('equal')\n",
    "    if Y is not None:\n",
    "        idx_tp = np.logical_and(T >= .5, Y >= .5) # True positives\n",
    "        idx_fp = np.logical_and(T < .5, Y >= .5) # False positives\n",
    "        idx_fn = np.logical_and(T >= .5, Y < .5) # False negatives\n",
    "        idx_tn = np.logical_and(T < .5, Y < .5) # True negatives\n",
    "        \n",
    "        plt.scatter(X[idx_tp,0], X[idx_tp,1], marker=m[0], c=\"red\", label=\"TP\")\n",
    "        plt.scatter(X[idx_fp,0], X[idx_fp,1], marker=m[1], c=\"red\", label=\"FP\")\n",
    "        plt.scatter(X[idx_fn,0], X[idx_fn,1], marker=m[0], c=\"blue\", label=\"FN\")\n",
    "        plt.scatter(X[idx_tn,0], X[idx_tn,1], marker=m[1], c=\"blue\", label=\"TN\")\n",
    "\n",
    "    else:\n",
    "        idx_pos, idx_neg = (T > .5), (T < .5)\n",
    "        plt.scatter(X[idx_pos,0], X[idx_pos,1], marker=m[0], color=\"blue\", label=\"Pos\")\n",
    "        plt.scatter(X[idx_neg,0], X[idx_neg,1], marker=m[1], color=\"blue\", label=\"Neg\")\n",
    "    \n",
    "    plt.xlabel(\"x\"); plt.ylabel(\"y\")\n",
    "    plt.legend(bbox_to_anchor=(0, 1), loc='upper left', ncol=1)\n",
    "    return fig.axes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = two_clusters(100, variance=0.15)\n",
    "visualize_dataset(X, T, Y=None);\n",
    "visualize_dataset(X, T, Y=np.random.uniform(size=T.size));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = two_rings(300, noise=0.01, separation=.3)\n",
    "visualize_dataset(X, T, Y=None);\n",
    "visualize_dataset(X, T, Y=np.random.uniform(size=T.size));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using linear regression for classification?\n",
    "\n",
    "We'll use the following convention: ${\\bf w}^{\\text{T}}{\\bf x} + b = 1$ for positive examples and ${\\bf w}^{\\text{T}}{\\bf x} +b = 0$ for negative examples.\n",
    "\n",
    "Actually: $\\hat{x} = [x_0, x_1, 1]$ and $y = {\\bf w}^{\\text{T}}\\hat{{\\bf x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear(X, T):\n",
    "    N = X.shape[0]\n",
    "    X_hat = np.concatenate([X, np.ones((N, 1))], axis=1)\n",
    "    W = np.dot(np.linalg.pinv(X_hat), T)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear(X, W):\n",
    "    N = X.shape[0]\n",
    "    X_hat = np.concatenate([X, np.ones((N, 1))], axis=1)\n",
    "    Y = np.dot(X_hat, W)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = two_clusters(100, variance=0.1)\n",
    "W = train_linear(X, T)\n",
    "Y = predict_linear(X, W)\n",
    "ax = visualize_dataset(X, T, Y);\n",
    "\n",
    "__x = np.linspace(0, 1, 100)\n",
    "__y = (.5 - __x * W[0] - W[2]) / W[1]\n",
    "\n",
    "plt.plot(__x, __y, c=\"black\", axes=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about adding some other examples that should pose no problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_N = 800\n",
    "extra_X = np.random.randn(extra_N, 2) * 0.1\n",
    "extra_X[:,1] += 2.5\n",
    "extra_X[:,0] += 3.5\n",
    "extra_T = np.ones(extra_N)\n",
    "X_full = np.concatenate([X, extra_X], axis=0)\n",
    "T_full = np.concatenate([T, extra_T], axis=0)\n",
    "\n",
    "ax = visualize_dataset(X_full, T_full, Y=None);\n",
    "__x = np.linspace(0, 1, 100)\n",
    "__y = (.5 - __x * W[0] - W[2]) / W[1]\n",
    "\n",
    "plt.plot(__x, __y, c=\"black\", axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the model on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_full = train_linear(X_full, T_full)\n",
    "Y = predict_linear(X_full, W_full)\n",
    "ax = visualize_dataset(X_full, T_full, Y);\n",
    "\n",
    "__x = np.linspace(0,2, 100)\n",
    "__y = (.5 - __x * W[0] - W[2]) / W[1]\n",
    "plt.plot(__x, __y, c=\"black\", axes=ax)\n",
    "\n",
    "__y = (.5 - __x * W_full[0] - W_full[2]) / W_full[1]\n",
    "plt.plot(__x, __y, c=\"purple\", axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The logistic function\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "plt.figure()\n",
    "plt.plot(x, logistic(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning models for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(Y, T):\n",
    "    idx0 = (T > 0.5)\n",
    "    idx1 = (T < 0.5)\n",
    "    return -np.sum(np.log(Y[idx0])) - np.sum(np.log(1.0 - Y[idx1]))\n",
    "\n",
    "def accuracy(Y, T):\n",
    "    N = Y.shape[0]\n",
    "    return np.sum(((T < 0.5) & (Y < 0.5)) | ((T > 0.5) & (Y > 0.5))) / N\n",
    "\n",
    "print(\"NLL:\", nll(np.random.uniform(size=(100)), T))\n",
    "print(\"Accuracy: \", accuracy(np.random.uniform(size=(100)), T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, T, train=.8):\n",
    "    N = X.shape[0]\n",
    "    N_train = int(round(N * train))\n",
    "    N_test = N - N_train\n",
    "\n",
    "    X_train, X_test = X[:N_train,:], X[N_train:,:]\n",
    "    T_train, T_test = T[:N_train], T[N_train:]\n",
    "    return X_train, T_train, X_test, T_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic(X, T, lr=.01, epochs_no=10):\n",
    "    # TODO <1> : Make 100 steps of SGD\n",
    "    (N, D) = X.shape\n",
    "    X_hat = np.concatenate([X, np.ones((N, 1))], axis=1)\n",
    "    W = np.random.randn((D+1))\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "def predict_logistic(X, W):\n",
    "    # TODO <2> : Compute the model's predictions\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = two_clusters(100, variance=0.1)\n",
    "W = train_logistic(X, T, lr=.1, epochs_no=1000)\n",
    "Y = predict_logistic(X, W)\n",
    "ax = visualize_dataset(X, T, Y);\n",
    "\n",
    "__x = np.linspace(0, 1, 100)\n",
    "__y = (-__x * W[0] - W[2]) / W[1]\n",
    "\n",
    "plt.plot(__x, __y, c=\"black\", axes=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_N = 800\n",
    "extra_X = np.random.randn(extra_N, 2) * 0.1\n",
    "extra_X[:,1] += 1.5\n",
    "extra_X[:,0] += 3.5\n",
    "extra_T = np.ones(extra_N)\n",
    "X_full = np.concatenate([X, extra_X], axis=0)\n",
    "T_full = np.concatenate([T, extra_T], axis=0)\n",
    "\n",
    "ax = visualize_dataset(X_full, T_full, Y=None);\n",
    "__x = np.linspace(0, 1, 100)\n",
    "__y = (.5 - __x * W[0] - W[2]) / W[1]\n",
    "\n",
    "plt.plot(__x, __y, c=\"black\", axes=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_full = train_logistic(X_full, T_full, lr=.1, epochs_no=10000)\n",
    "Y = predict_logistic(X_full, W_full)\n",
    "ax = visualize_dataset(X_full, T_full, Y);\n",
    "\n",
    "__x = np.linspace(0, 1, 100)\n",
    "__y = (- __x * W[0] - W[2]) / W[1]\n",
    "plt.plot(__x, __y, c=\"black\", axes=ax)\n",
    "\n",
    "__y = (- __x * W_full[0] - W_full[2]) / W_full[1]\n",
    "plt.plot(__x, __y, c=\"purple\", axes=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_full(X, T, lr=.01, epochs_no=100):\n",
    "    (N, D) = X.shape\n",
    "    X1 = np.concatenate([np.ones((N, 1)), X], axis=1)\n",
    "    W = np.random.randn((D+1))\n",
    "\n",
    "    X_train, T_train, X_test, T_test = split_dataset(X1, T)\n",
    "\n",
    "    train_acc, test_acc = [], []\n",
    "    train_nll, test_nll = [], []\n",
    "    W_trace = [W.copy()]\n",
    "\n",
    "    for epoch in range(epochs_no):\n",
    "        Y_train = logistic(np.dot(X_train, W))\n",
    "        # Update parameters\n",
    "        W -= lr * np.dot(X_train.T, Y_train-T_train)\n",
    "\n",
    "        # Just for plotting\n",
    "        Y_test = 1. / (1. + np.exp(-np.dot(X_test, W)))\n",
    "        train_acc.append(accuracy(Y_train, T_train))\n",
    "        test_acc.append(accuracy(Y_test, T_test))\n",
    "        train_nll.append(nll(Y_train, T_train))\n",
    "        test_nll.append(nll(Y_test, T_test))\n",
    "        W_trace.append(W.copy())\n",
    "        \n",
    "    return W, train_acc, test_acc, train_nll, test_nll, W_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolution(train_acc, test_acc, train_nll, test_nll):\n",
    "    epochs_no = len(train_acc)\n",
    "    fig, (ax1, ax2) = plt.subplots(2,1);\n",
    "    ax1.plot(range(epochs_no), train_acc, sns.xkcd_rgb[\"green\"], label=\"Train Accuracy\")\n",
    "    ax1.plot(range(epochs_no), test_acc, sns.xkcd_rgb[\"red\"], label=\"Test Accuracy\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.legend(loc='lower right', ncol=1)\n",
    "\n",
    "    ax2.plot(range(epochs_no), train_nll, sns.xkcd_rgb[\"green\"], label=\"Train NLL\")\n",
    "    ax2.plot(range(epochs_no), test_nll, sns.xkcd_rgb[\"red\"], label=\"Test NLL\")\n",
    "    ax2.set_xlabel(\"iteration\")\n",
    "    ax2.set_ylabel(\"NLL\")\n",
    "    ax2.legend(loc='upper right', ncol=1);\n",
    "    return (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_NO = 2000\n",
    "results = train_logistic_full(X, T, lr=.01, epochs_no=EPOCHS_NO)\n",
    "\n",
    "W, train_acc, test_acc, train_nll, test_nll, W_trace = results\n",
    "plot_evolution(train_acc, test_acc, train_nll, test_nll);\n",
    "\n",
    "\n",
    "del EPOCHS_NO, results, W, train_acc, test_acc, train_nll, test_nll, W_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the second dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = two_rings(300, separation=.4)\n",
    "W = train_logistic(X, T, lr=.1, epochs_no=1000)\n",
    "Y = predict_logistic(X, W)\n",
    "ax = visualize_dataset(X, T, Y);\n",
    "\n",
    "__x = np.linspace(-1, 1, 100)\n",
    "__y = (-__x * W[0] - W[2]) / W[1]\n",
    "\n",
    "plt.plot(__x, __y, c=\"black\", axes=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
